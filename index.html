<!doctype html>

<html lang="en">
  <head>
    <meta charset="utf-8" />

    <title>DOCCI</title>
    <meta name="description" content="DOCCI" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Material+Icons&display=block"
      rel="stylesheet"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css"
    />
    <link rel="stylesheet" href="style.css" />
  </head>

  <body onload="loadCallback();">
    <div class="mask" id="start-mask"></div>
    <div class="wrap">
      <div class="center block">
        <h1>DOCCI</h1>
        <h2 class="subtitle">Descriptions of Connected and Contrasting Images</h2>
        <div>
          <a href="https://yasumasaonoe.github.io/" target="_blank">Yasumasa Onoe</a><sup>1</sup>,
          <a href="" target="_blank">Sunayana Rane</a><sup>2</sup>,
          <a href="https://zackberger.com/" target="_blank">Zachary Berger</a><sup>1</sup>,
          <a href="https://yonatanbitton.github.io/" target="_blank">Yonatan Bitton</a><sup>1</sup>,
          <a href="https://j-min.io/" target="_blank">Jaemin Cho</a><sup>3</sup>,
          <a href="https://www.roopalgarg.com/" target="_blank">Roopal Garg</a><sup>1</sup>,<br />
          <a href="https://alexyku.github.io/" target="_blank">Alexander Ku</a><sup>2,1</sup>,
          <a href="https://jponttuset.cat" target="_blank">Jordi Pont-Tuset</a><sup>1</sup>,
          <a href="https://scholar.google.com/citations?user=QMe5H7kAAAAJ&hl=en" target="_blank"
            >Zarana Parekh</a
          ><sup>1</sup>,
          <a href="https://scholar.google.com/citations?user=Wg58ZR4AAAAJ&hl=en" target="_blank"
            >Garrett Tanzer</a
          ><sup>1</sup>,
          <a href="https://research.google/people/su-wang/" target="_blank">Su Wang</a><sup>1</sup>,
          <a href="https://scholar.google.co.uk/citations?user=TP_JZm8AAAAJ&hl=en" target="_blank"
            >Jason Baldridge</a
          ><sup>1</sup><br />
          <sup>1</sup>Google, <sup>2</sup>Princeton University, <sup>3</sup>UNC Chapel Hill
        </div>
        <div class="button-bar">
          <a href="./viz.html?&c=" target="_blank">
            <span class="button"
              ><span class="button-wrapper"
                ><i class="fa fa-magnifying-glass"></i> Explore</span
              ></span
            ></a
          >
          <a href="#downloads">
            <span class="button"
              ><span class="button-wrapper"><i class="fa fa-download"></i> Downloads</span></span
            ></a
          >
          <a href="" target="_blank"
            ><span class="button"
              ><span class="button-wrapper"><i class="fa fa-file-pdf"></i> PDF</span></span
            ></a
          >
          <a href="" target="_blank"
            ><span class="button"
              ><span class="button-wrapper"
                ><svg
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 74.492 100.25"
                  style="width: 14px"
                >
                  <g>
                    <path
                      d="M586.72,255.616a3.377,3.377,0,0,1,.448.031,5.917,5.917,0,0,1,3.581,2.79c.454,1.116.314,2.023-1.315,4.141L563.168,293.6l-8.558-10.047,29.348-26.616a4.406,4.406,0,0,1,2.762-1.321m0-1.5a5.766,5.766,0,0,0-3.69,1.643l-.041.032-.038.035L553.6,282.442l-1.077.977.943,1.107,8.558,10.047,1.145,1.344,1.141-1.348,26.267-31.022.022-.027.022-.028c1.574-2.046,2.327-3.622,1.516-5.619a7.309,7.309,0,0,0-4.779-3.714,5.083,5.083,0,0,0-.64-.043Z"
                      transform="translate(-526.086 -245.559)"
                      fill="#fff"
                    />
                    <path
                      d="M553.423,284.593l8.977,10.558L597.911,337.9c.873,1.093,1.419,2.186,1.047,3.418a4.092,4.092,0,0,1-2.721,2.837,3.557,3.557,0,0,1-1.045.159,4,4,0,0,1-2.687-1.124L548.01,300.808c-3.5-3.5-2.971-8.151.436-11.558l4.977-4.657m.124-2.17L552.4,283.5l-4.976,4.656c-4.192,4.191-4.372,9.816-.473,13.714l44.521,42.4a5.485,5.485,0,0,0,3.722,1.538,5.1,5.1,0,0,0,1.483-.224,5.59,5.59,0,0,0,3.719-3.838,5.176,5.176,0,0,0-1.31-4.788l-35.53-42.767-8.988-10.571-1.019-1.2Z"
                      transform="translate(-526.086 -245.559)"
                      fill="#fff"
                    />
                    <path
                      d="M562.4,295.151l9.556,11.5,5.761-5.356a7.926,7.926,0,0,0,.041-11.743l-43.7-41.923s-1.671-2.029-3.437-2.071a4.49,4.49,0,0,0-4.23,2.718c-.688,1.651-.194,2.809,1.315,4.97l29.306,35.565Z"
                      transform="translate(-526.086 -245.559)"
                      fill="#fff"
                    />
                    <path
                      d="M553.7,306.223l-17.116,21.024c-1.255,1.337-2.032,3.683-1.331,5.367a4.587,4.587,0,0,0,4.287,2.841,4.087,4.087,0,0,0,3.082-1.523l20.328-18.9Z"
                      transform="translate(-526.086 -245.559)"
                      fill="#fff"
                    />
                    <path
                      d="M592.074,250.547"
                      transform="translate(-526.086 -245.559)"
                      fill="#fff"
                      stroke="#000"
                      stroke-miterlimit="10"
                      stroke-width="0.25"
                    />
                  </g>
                </svg>
                arXiv</span
              ></span
            ></a
          >
          <a href="javascript:document.getElementById('bibtex').open = true;"
            ><span class="button"
              ><span class="button-wrapper"
                ><i class="fa-brands fa-google-scholar"></i> BibTex</span
              ></span
            ></a
          >
        </div>
      </div>
      <mwc-dialog id="bibtex">
        <pre id="bibtex_text">
@inproceedings{OnoeDocci2024,
  author        = {Yasumasa Onoe and Sunayana Rane and Zachary Berger and Yonatan Bitton and Jaemin Cho and Roopal Garg and
    Alexander Ku and Zarana Parekh and Jordi Pont-Tuset and Garrett Tanzer and Su Wang and Jason Baldridge},
  title         = {{DOCCI: Descriptions of Connected and Contrasting Images}},
  booktitle     = {arXiv},
  year          = {2024}
}</pre
        >
        <mwc-button
          slot="secondaryAction"
          onclick="navigator.clipboard.writeText(document.getElementById('bibtex_text').innerText);"
        >
          Copy to clipboard
        </mwc-button>
        <mwc-button
          slot="primaryAction"
          onclick="javascript:document.getElementById('bibtex').open = false;"
        >
          Close
        </mwc-button>
      </mwc-dialog>

      <hr class="separator" />
      <h2>Abstract</h2>
      <div class="justify block">
        Vision-language datasets are vital for both text-to-image (T2I) and image-to-text (I2T)
        research. However, current datasets lack descriptions with fine-grained detail that would
        allow for richer associations to be learned by models. To fill the gap, we introduce
        Descriptions of Connected and Contrasting Images (DOCCI), a dataset with long,
        human-annotated English descriptions for 15k images that were taken, curated and donated by
        a single researcher intent on capturing key challenges such as spatial relations, counting,
        text rendering, world knowledge, and more. We instruct human annotators to create
        comprehensive descriptions for each image; these average 136 words in length and are crafted
        to clearly distinguish each image from those that are related or similar. Each description
        is highly compositional and typically encompasses multiple challenges. Through both
        quantitative and qualitative analyses, we demonstrate that DOCCI serves as an effective
        training resource for image-to-text generation -- a PaLI 5B model finetuned on DOCCI shows
        equal or superior results compared to highly-performant larger models like LLaVA-1.5 7B and
        InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for text-to-image
        generation, highlighting the limitations of current text-to-image models in capturing long
        descriptions and fine details.
      </div>
      <hr class="separator" />
      <h2>Dataset</h2>
      <div class="center block">
        <p class="justify">
          The DOCCI dataset consists of comprehensive descriptions on 15k images specifically taken
          with the objective of evaluating T2I and I2T models. These cover a lot of key details in
          the images, as illustrated below.
        </p>
        <img style="margin: 20px 0" src="web-data/fig1.png" />
        <p class="justify">
          This unprecedented level of detail in the descriptions allows to evaluate T2I models by
          messuring how well they reconstruct the real images, as seen in the example below.
        </p>
        <img style="margin: 20px 0; max-width: 730px" src="web-data/fig2.png" />
      </div>
      <hr class="separator" />
      <h2 id="downloads">Downloads</h2>
      <div class="center block last">
        <p class="center">
          ðŸ¤— DOCCI can also be used via
          <a href="https://huggingface.co/datasets/google/docci" target="_blank"
            >Huggingface Datasets</a
          >
          ðŸ¤—
        </p>
        <div>
          <a href="https://storage.googleapis.com/docci/data/docci_descriptions.jsonlines"
            ><span class="button wide" style="margin-top: 25px"
              ><span class="button-wrapper"
                ><i class="fa fa-download"></i> DOCCI Descriptions (10.5 MB)</span
              ></span
            ></a
          >
          <p class="justify paragraph">
            Detailed descriptions of each of the DOCCI images. The format is
            <a href="https://jsonlines.org/" target="_blank">JSON Lines</a>, which means that each
            line is a JSON-encoded string. The description of each of the fields in the JSON object
            can be found in <a href="javascript:openReadme();">this README</a>. If you find any
            error in the annotations,
            <a href="https://forms.gle/5R8Ab76JRCD88MKy6" target="_blank"
              >please report it in this form</a
            >.
          </p>
        </div>
        <div>
          <a href="https://storage.googleapis.com/docci/data/docci_images.tar.gz"
            ><span class="button wide"
              ><span class="button-wrapper"
                ><i class="fa fa-download"></i> DOCCI Images (7.1 GB)</span
              ></span
            ></a
          >
          <p class="justify paragraph">
            15k images that form the visual basis for the DOCCI descriptions (all either portrait or
            landscape aspect ratio).
          </p>
        </div>
        <div>
          <a href="https://storage.googleapis.com/docci/data/docci_metadata.jsonlines"
            ><span class="button wide"
              ><span class="button-wrapper"
                ><i class="fa fa-download"></i> DOCCI Metadata (155 MB)</span
              ></span
            ></a
          >
          <p class="justify paragraph">
            Metadata about the images and the prompts as
            <a href="https://jsonlines.org/" target="_blank">JSON Lines</a>. The description of each
            of the fields in the JSON object can be found in
            <a href="javascript:openReadme()">this README</a>.
          </p>
        </div>
        <div>
          <a href="https://storage.googleapis.com/docci/data/docci_images_aar.tar.gz"
            ><span class="button wide"
              ><span class="button-wrapper"
                ><i class="fa fa-download"></i> DOCCI AAR Images (3.6 GB)</span
              ></span
            ></a
          >
          <p class="justify paragraph">
            An additional 9.6k unannotated images that are cropped to arbitrary aspect ratios from
            tall portraits to long landscapes.
          </p>
        </div>
        <p class="license">
          <i
            >The annotations and images are licensed by Google LLC under
            <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC BY 4.0</a>
            license.</i
          >
        </p>
      </div>
      <mwc-dialog id="readme">
        <h3>Descriptions</h3>
        <table>
          <thead>
            <tr>
              <th>Field</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>example_id</code></td>
              <td>
                The unique ID of an example follows this format:
                <code>&lt;split_id&gt;_&lt;index&gt;</code>.
              </td>
            </tr>
            <tr>
              <td><code>split</code></td>
              <td>Identifier of the data split it belongs to (e.g. <code>qual_dev</code>).</td>
            </tr>

            <tr>
              <td><code>image_file</code></td>
              <td>Image filename (JPEG format).</td>
            </tr>
            <tr>
              <td><code>description</code></td>
              <td>Text description of the associated image.</td>
            </tr>
          </tbody>
        </table>
        <h3>Metadata</h3>
        <table>
          <thead>
            <tr>
              <th>Field</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>example_id</code></td>
              <td>
                The unique ID of an example follows this format:
                <code>&lt;split_id&gt;_&lt;index&gt;</code>.
              </td>
            </tr>
            <tr>
              <td><code>cluster_id</code></td>
              <td>Cluster ID to which an image belongs.</td>
            </tr>
            <tr>
              <td><code>entity_tags</code></td>
              <td>
                Tags of common entities across DOCCI, such as <code>#E_CAT_TIGER</code>, which
                refers to the same gray tabby cat that appears many times in DOCCI.
              </td>
            </tr>
            <tr>
              <td><code>image_width</code></td>
              <td>Size of the image in pixels (width).</td>
            </tr>
            <tr>
              <td><code>image_height</code></td>
              <td>Size of the image in pixels (height).</td>
            </tr>
            <tr>
              <td><code>cloud_vision_api_responses</code></td>
              <td>
                Responses from
                <a href="https://cloud.google.com/vision/docs">Google Cloud Vision API</a> such as
                object detection, OCR, and SafeSearch.
              </td>
            </tr>
            <tr>
              <td><code>dsg</code></td>
              <td>
                DSG subquestions and results. See the
                <a href="https://google.github.io/dsg/">DSG paper</a> for more details. These are
                only available for the <code>test</code>, <code>qual_dev</code>, and
                <code>qual_test</code> splits.
              </td>
            </tr>
            <tr>
              <td><code>distractors</code></td>
              <td>
                File names of four visually similar images. These are available for a randomly
                selected set of 1,000 test examples. For more details, see Section 3.3 of our paper.
              </td>
            </tr>
          </tbody>
        </table>
        <mwc-button slot="primaryAction" onclick="closeReadme();"> Close </mwc-button>
      </mwc-dialog>
    </div>
    <script type="text/javascript" src="web_code.js"></script>
  </body>
</html>
